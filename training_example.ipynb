{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import pickle\n",
    "from utils import *\n",
    "from dense_neural_class import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagens de treino: (60000, 28, 28)\n",
      "R贸tulos de treino: (60000,)\n",
      "Imagens de teste: (10000, 28, 28)\n",
      "R贸tulos de test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Reads the MNIST image file and returns a NumPy array with the images.\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape(num_images, rows, cols)\n",
    "    return images\n",
    "\n",
    "# Reads the MNIST label file and returns a NumPy array with the labels.\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "# Load train set\n",
    "train_images = load_mnist_images('./mnist/train-images.idx3-ubyte')\n",
    "train_labels = load_mnist_labels('./mnist/train-labels.idx1-ubyte')\n",
    "# Load test set\n",
    "test_images = load_mnist_images('./mnist/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('./mnist/t10k-labels.idx1-ubyte')\n",
    "# Check shapes\n",
    "print(f\"Imagens de treino: {train_images.shape}\")  # Ex.: (60000, 28, 28)\n",
    "print(f\"R贸tulos de treino: {train_labels.shape}\")\n",
    "print(f\"Imagens de teste: {test_images.shape}\")# Ex.: (60000,)\n",
    "print(f\"R贸tulos de test: {test_labels.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the data in a best-known format.\n",
    "\n",
    "# Train set\n",
    "X = train_images\n",
    "X = X.reshape(-1,28*28)\n",
    "Y = train_labels\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "# Test set\n",
    "X_test = test_images\n",
    "X_test = X_test.reshape(-1,28*28)\n",
    "Y_test = test_labels\n",
    "Y_test = Y_test.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of the neural network as model2.\n",
    "model2 = Dense_Neural_Diy(input_size=784, hidden_layer1_size=50, hidden_layer2_size=20 , output_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model First Training\n",
    "Deliberately, we will perform a poor initial training by using a batch size of 60,000, which is the total number of training samples. This way, the algorithm would be equivalent to standard gradient descent, which is an excellent algorithm but not suitable for large datasets, as it converges very slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cost: 2.3058211101136354\n",
      "Epoch 10, cost: 2.2872471812774173\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X,Y,learning_rate=0.005, epochs=11, batch_size=60000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test: 0.1434\n",
      "Accuracy on Train: 0.13586666666666666\n"
     ]
    }
   ],
   "source": [
    "# Prediction using test set\n",
    "Y_pred_test = model2.predict(X_test).reshape(-1,1)\n",
    "# Prediction using train set\n",
    "Y_hat = model2.predict(X).reshape(-1,1).reshape(-1,1)\n",
    "\n",
    "print(f'Accuracy on Test: {np.mean(Y_test == Y_pred_test)}')\n",
    "print(f'Accuracy on Train: {np.mean(Y == Y_hat)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Second Training\n",
    "Using a small batch size significantly improves the convergence speed of the algorithm. Despite using 61 epochs now, it is evident that the algorithm quickly reduces the cost in the early epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cost: 0.14221218844165667\n",
      "Epoch 10, cost: 0.04178002568077196\n",
      "Epoch 20, cost: 0.02614798888615986\n",
      "Epoch 30, cost: 0.010887485703809732\n",
      "Epoch 40, cost: 0.016473049941888696\n",
      "Epoch 50, cost: 0.009286794087354359\n",
      "Epoch 60, cost: 0.0001825332869148346\n"
     ]
    }
   ],
   "source": [
    "model2.improve_train(X,Y, learning_rate=0.005, epochs=61, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test: 0.9744\n",
      "Accuracy on Train: 0.9999666666666667\n"
     ]
    }
   ],
   "source": [
    "# Prediction using test set\n",
    "Y_pred_test = model2.predict(X_test).reshape(-1,1)\n",
    "# Prediction using train set\n",
    "Y_hat = model2.predict(X).reshape(-1,1).reshape(-1,1)\n",
    "\n",
    "print(f'Accuracy on Test: {np.mean(Y_test == Y_pred_test)}')\n",
    "print(f'Accuracy on Train: {np.mean(Y == Y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The accuracy has improved dramatically in both training and testing.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "save_model('model_save_test', model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test: 0.9744\n",
      "Accuracy on Train: 0.9999666666666667\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "loaded_model = load_model('model_save_test')\n",
    "\n",
    "# Evaluating\n",
    "# Prediction using test set\n",
    "Y_pred_test = loaded_model.predict(X_test).reshape(-1,1)\n",
    "# Prediction using train set\n",
    "Y_hat = loaded_model.predict(X).reshape(-1,1).reshape(-1,1)\n",
    "\n",
    "print(f'Accuracy on Test: {np.mean(Y_test == Y_pred_test)}')\n",
    "print(f'Accuracy on Train: {np.mean(Y == Y_hat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
